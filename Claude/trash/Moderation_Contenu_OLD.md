# üõ°Ô∏è Mod√©ration du Contenu - INTUITION Founders Totem

**Date de cr√©ation** : 18 novembre 2025
**Derni√®re mise √† jour** : 18 novembre 2025
**Statut** : ‚úÖ Complet

---

## üìã Table des mati√®res

1. [Introduction](#-introduction)
2. [Enjeux et risques](#-enjeux-et-risques)
3. [Strat√©gie de mod√©ration](#-strat√©gie-de-mod√©ration)
4. [Solutions techniques](#-solutions-techniques)
5. [Impl√©mentation recommand√©e](#-impl√©mentation-recommand√©e)
6. [Workflow de mod√©ration](#-workflow-de-mod√©ration)
7. [Interface de mod√©ration manuelle](#-interface-de-mod√©ration-manuelle)
8. [Co√ªts et maintenance](#-co√ªts-et-maintenance)
9. [Conformit√© l√©gale](#-conformit√©-l√©gale)
10. [Plan d'impl√©mentation](#-plan-dimpl√©mentation)

---

## üéØ Introduction

La plateforme INTUITION Founders Totem permet aux utilisateurs de proposer des totems (objets, animaux, traits, √©nergies) pour repr√©senter les 42 fondateurs. Cette libert√© cr√©ative n√©cessite un syst√®me de mod√©ration robuste pour :

- **Prot√©ger la r√©putation** des fondateurs et d'INTUITION
- **Maintenir un environnement respectueux** pour la communaut√©
- **√âviter le contenu offensant** (insultes, propos haineux, contenu sexuel inappropri√©, etc.)
- **Se conformer aux r√©glementations** (GDPR, Digital Services Act, etc.)

### üéØ Objectifs de la mod√©ration

1. **Pr√©vention** : Bloquer le contenu inappropri√© avant publication
2. **D√©tection** : Identifier automatiquement le contenu suspect
3. **Action** : Workflow manuel pour valider ou rejeter les contenus flagg√©s
4. **Transparence** : Notifier les utilisateurs des d√©cisions de mod√©ration
5. **Apprentissage** : Am√©liorer le syst√®me au fil du temps

---

## ‚ö†Ô∏è Enjeux et risques

### Risques identifi√©s

| Risque | Gravit√© | Probabilit√© | Impact |
|--------|---------|-------------|---------|
| **Insultes envers les fondateurs** | üî¥ Critique | Moyenne | R√©putation, l√©gal |
| **Contenu haineux / discriminatoire** | üî¥ Critique | Faible | L√©gal, bannissement |
| **Spam et publicit√©** | üü° Mod√©r√©e | √âlev√©e | Qualit√©, UX |
| **Contenu sexuel explicite** | üî¥ Critique | Faible | R√©putation, l√©gal |
| **Contournement de filtres (l33t, espaces)** | üü° Mod√©r√©e | Moyenne | Qualit√© |
| **Faux positifs (blocages l√©gitimes)** | üü° Mod√©r√©e | Moyenne | UX, frustration |

### Types de contenu √† mod√©rer

#### 1. **Langage offensant**
- Insultes, jurons
- Propos haineux (racisme, sexisme, homophobie, etc.)
- Harc√®lement et menaces

#### 2. **Contenu sexuel**
- Contenu pornographique ou explicite
- R√©f√©rences sexuelles inappropri√©es
- Harc√®lement sexuel

#### 3. **Spam et abus**
- Publicit√© non sollicit√©e
- Liens suspects ou malveillants
- Contenu r√©p√©titif

#### 4. **D√©sinformation**
- Fausses informations sur les fondateurs
- Usurpation d'identit√©

#### 5. **Contournement**
- L33t speak (l0l, 4ss, etc.)
- Espaces entre lettres (a s s)
- Caract√®res Unicode similaires

---

## üéØ Strat√©gie de mod√©ration

Nous adoptons une **approche hybride** combinant :

### 1. **Mod√©ration automatique (Tier 1)**
- Filtrage en temps r√©el des mots interdits
- D√©tection d'obfuscation (l33t, espaces)
- Analyse de sentiment et toxicit√© (optionnel)

### 2. **Mod√©ration semi-automatique (Tier 2)**
- Mise en file d'attente des contenus suspects
- Notation de confiance (score de toxicit√©)
- Priorisation des cas urgents

### 3. **Mod√©ration manuelle (Tier 3)**
- Review humaine des contenus flagg√©s
- D√©cisions finales : Approuver / Rejeter / Bannir
- Feedback pour am√©liorer les filtres

### 4. **Mod√©ration communautaire (Tier 4 - Futur)**
- Syst√®me de signalement par les utilisateurs
- Votes communautaires
- R√©putation des mod√©rateurs

---

## üõ†Ô∏è Solutions techniques

### Comparaison des approches

| Approche | Avantages | Inconv√©nients | Co√ªt | Recommandation |
|----------|-----------|---------------|------|----------------|
| **Listes de mots (npm packages)** | ‚úÖ Gratuit<br>‚úÖ Simple<br>‚úÖ Multilingue<br>‚úÖ Offline | ‚ö†Ô∏è Faux positifs<br>‚ö†Ô∏è Contournement facile<br>‚ö†Ô∏è Maintenance | $0 | ‚úÖ **OUI - Base** |
| **IA/NLP (OpenAI, Perspective API)** | ‚úÖ Contexte<br>‚úÖ Pr√©cision<br>‚úÖ D√©tection subtile | ‚ö†Ô∏è Co√ªt API<br>‚ö†Ô∏è Latence<br>‚ö†Ô∏è D√©pendance externe | $10-100/mois | üü° **Optionnel - Phase 2** |
| **AWS Comprehend Toxicity** | ‚úÖ Int√©gration AWS<br>‚úÖ 7 cat√©gories<br>‚úÖ Scores de confiance | ‚ö†Ô∏è Anglais uniquement<br>‚ö†Ô∏è Co√ªt<br>‚ö†Ô∏è Faux positifs SEXUAL | $0.0001/unit√© | üü° **Optionnel** |
| **Custom ML Model** | ‚úÖ Personnalis√©<br>‚úÖ Pas de co√ªt API | ‚ö†Ô∏è Complexit√© extr√™me<br>‚ö†Ô∏è Maintenance<br>‚ö†Ô∏è Dataset | $0 (mais temps) | ‚ùå **NON - Overkill** |

---

## üì¶ Packages npm recommand√©s

### ü•á Option 1 : **glin-profanity** (Recommand√©)

**Pourquoi ?**
- ‚úÖ **20+ langues** (FR, EN inclus)
- ‚úÖ **D√©tection d'obfuscation** (l33t, espaces, Unicode)
- ‚úÖ **Niveaux de s√©v√©rit√©** (faible, mod√©r√©, s√©v√®re)
- ‚úÖ **TypeScript natif**
- ‚úÖ **Hooks React** disponibles
- ‚úÖ **Actif et maintenu**

**Installation** :
```bash
pnpm add glin-profanity
```

**Usage** :
```typescript
import { ProfanityEngine } from 'glin-profanity';

const profanityEngine = new ProfanityEngine({
  languages: ['en', 'fr'],
  grawlixChar: '*',
  sensitivity: 'high', // low, medium, high
  obfuscationDetection: true
});

const result = profanityEngine.analyze('texte √† analyser');

console.log(result.isProfane); // true/false
console.log(result.score); // 0-1
console.log(result.matches); // Mots d√©tect√©s
console.log(result.severity); // mild, moderate, severe
```

**Avec React** :
```typescript
import { useProfanityFilter } from 'glin-profanity/react';

function TotemForm() {
  const { checkText, isProfane, censorText } = useProfanityFilter({
    languages: ['en', 'fr'],
    sensitivity: 'high'
  });

  const handleSubmit = (name: string) => {
    if (isProfane(name)) {
      toast.error('Le nom contient du contenu inappropri√©');
      return;
    }
    // Soumettre...
  };
}
```

---

### ü•à Option 2 : **@2toad/profanity**

**Pourquoi ?**
- ‚úÖ **TypeScript natif**
- ‚úÖ **Multi-langues** (EN, FR, DE)
- ‚úÖ **Simple et l√©ger**
- ‚úÖ **Whitelist** (exceptions)

**Installation** :
```bash
pnpm add @2toad/profanity
```

**Usage** :
```typescript
import { profanity } from '@2toad/profanity';

profanity.setLanguages(['en', 'fr']);

profanity.exists('texte'); // true/false
profanity.censor('texte'); // Remplace par ***
profanity.whitelist(['Phoenix', 'Arsenal']); // Exceptions
```

---

### ü•â Option 3 : **leo-profanity**

**Pourquoi ?**
- ‚úÖ **Multi-langues** (EN, FR, RU)
- ‚úÖ **Bas√© sur Shutterstock dictionary**
- ‚úÖ **Dictionnaires personnalisables**
- ‚úÖ **Populaire** (500k+ t√©l√©chargements/semaine)

**Installation** :
```bash
pnpm add leo-profanity
```

**Usage** :
```typescript
import * as filter from 'leo-profanity';

filter.loadDictionary('fr');
filter.check('texte'); // true/false
filter.clean('texte'); // Nettoie
filter.add(['motperso']); // Ajouter mots
filter.remove(['faux-positif']); // Supprimer mots
```

---

### ü§ñ Option 4 : **content-checker** (AI-powered)

**Pourquoi ?**
- ‚úÖ **D√©tection contextuelle** avec LLM
- ‚úÖ **Variantes de mots** d√©tect√©es
- ‚úÖ **Intentions malveillantes** d√©tect√©es
- ‚úÖ **TypeScript + ES6**

**Installation** :
```bash
pnpm add content-checker
```

**Usage** :
```typescript
import { ContentChecker } from 'content-checker';

const checker = new ContentChecker({
  useAI: true, // Utilise LLM pour contexte
  apiKey: process.env.OPENAI_API_KEY // Optionnel
});

const result = await checker.checkText('texte √† analyser');

console.log(result.isClean); // true/false
console.log(result.issues); // Liste des probl√®mes d√©tect√©s
```

‚ö†Ô∏è **Note** : N√©cessite une API key OpenAI si `useAI: true` ‚Üí Co√ªt API.

---

## üîß APIs de mod√©ration (Optionnel - Phase 2)

### üåê Perspective API (Google Jigsaw)

**Pourquoi ?**
- ‚úÖ **Gratuit** pour usage mod√©r√©
- ‚úÖ **18 langues** support√©es
- ‚úÖ **Score de toxicit√©** (0-1)
- ‚úÖ **Utilis√© par NYT, Reddit, WSJ**
- ‚úÖ **2 milliards d'analyses/jour**

**Limitations** :
- ‚ö†Ô∏è 100ms de latence par requ√™te
- ‚ö†Ô∏è Faux positifs/n√©gatifs possibles
- ‚ö†Ô∏è Quota API √† v√©rifier

**Obtenir une API key** :
1. Aller sur https://perspectiveapi.com/
2. Cr√©er un projet Google Cloud
3. Activer Perspective API
4. Copier l'API key

**Usage** :
```typescript
import { PerspectiveAPI } from '@conversationai/perspectiveapi-js-client';

const client = new PerspectiveAPI({
  apiKey: process.env.PERSPECTIVE_API_KEY
});

const result = await client.analyze({
  comment: { text: 'texte √† analyser' },
  languages: ['fr', 'en'],
  requestedAttributes: {
    TOXICITY: {},
    SEVERE_TOXICITY: {},
    IDENTITY_ATTACK: {},
    INSULT: {},
    PROFANITY: {},
    THREAT: {}
  }
});

const toxicityScore = result.attributeScores.TOXICITY.summaryScore.value;

if (toxicityScore > 0.7) {
  // Contenu suspect
}
```

**Cat√©gories disponibles** :
- `TOXICITY` : Toxicit√© g√©n√©rale
- `SEVERE_TOXICITY` : Toxicit√© s√©v√®re
- `IDENTITY_ATTACK` : Attaque identitaire
- `INSULT` : Insulte
- `PROFANITY` : Langage grossier
- `THREAT` : Menace
- `OBSCENE` : Obsc√®ne (exp√©rimental)
- `SPAM` : Spam (exp√©rimental)

---

### ‚òÅÔ∏è AWS Comprehend Toxicity Detection

**Pourquoi ?**
- ‚úÖ **7 cat√©gories** sp√©cifiques
- ‚úÖ **Scores de confiance**
- ‚úÖ **Int√©gration AWS native**

**Limitations** :
- ‚ö†Ô∏è **Anglais uniquement** (dealbreaker pour FR)
- ‚ö†Ô∏è Faux positifs sur SEXUAL
- ‚ö†Ô∏è Co√ªt : $0.0001/unit√© (100 caract√®res)

**Cat√©gories** :
1. PROFANITY (jurons)
2. HATE_SPEECH (discours haineux)
3. INSULT (insultes)
4. GRAPHIC (violence graphique)
5. HARASSMENT_OR_ABUSE (harc√®lement)
6. SEXUAL (contenu sexuel)
7. VIOLENCE_OR_THREAT (violence/menaces)

**Usage** :
```typescript
import { ComprehendClient, DetectToxicContentCommand } from '@aws-sdk/client-comprehend';

const client = new ComprehendClient({ region: 'us-east-1' });

const command = new DetectToxicContentCommand({
  TextSegments: [
    { Text: 'texte √† analyser' }
  ],
  LanguageCode: 'en'
});

const response = await client.send(command);

const toxicityScore = response.ResultList[0].Toxicity;
const labels = response.ResultList[0].Labels; // PROFANITY, INSULT, etc.
```

**Verdict** : ‚ùå **Non recommand√©** car pas de support fran√ßais.

---

### ü§ñ OpenAI Moderation API

**Pourquoi ?**
- ‚úÖ **Gratuit** (pour les clients OpenAI)
- ‚úÖ **Contextuel** et pr√©cis
- ‚úÖ **Multilingue** (d√©tection automatique)
- ‚úÖ **Cat√©gories d√©taill√©es**

**Cat√©gories** :
- `hate` : Contenu haineux
- `hate/threatening` : Menaces haineuses
- `harassment` : Harc√®lement
- `harassment/threatening` : Harc√®lement mena√ßant
- `self-harm` : Auto-mutilation
- `sexual` : Contenu sexuel
- `sexual/minors` : Contenu impliquant des mineurs
- `violence` : Violence
- `violence/graphic` : Violence graphique

**Usage** :
```typescript
import OpenAI from 'openai';

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY
});

const moderation = await openai.moderations.create({
  input: 'texte √† analyser'
});

const result = moderation.results[0];

console.log(result.flagged); // true/false
console.log(result.categories); // Cat√©gories d√©tect√©es
console.log(result.category_scores); // Scores (0-1)

if (result.categories.hate || result.categories.harassment) {
  // Bloquer
}
```

**Co√ªt** :
- ‚úÖ **Gratuit** si vous utilisez d√©j√† OpenAI
- ‚ö†Ô∏è Sinon : $0.02/1000 tokens (tr√®s abordable)

---

## üèÜ Impl√©mentation recommand√©e

### Phase 1 : MVP (Gratuit)

**Stack** :
- **Frontend** : `glin-profanity` avec React hooks
- **Backend** : `glin-profanity` pour validation serveur
- **D√©tection** : Obfuscation + sensibilit√© haute
- **Langues** : Fran√ßais + Anglais

**Workflow** :
1. ‚úÖ Utilisateur saisit le nom du totem
2. ‚úÖ Validation frontend en temps r√©el (feedback instant)
3. ‚úÖ Validation backend avant cr√©ation Atom
4. ‚úÖ Si flagg√© : Rejet avec message clair
5. ‚úÖ Si OK : Cr√©ation Atom + Triple

**Co√ªt** : **$0/mois**

---

### Phase 2 : Production (Optionnel)

**Stack** :
- **Frontend** : `glin-profanity` (filtrage rapide)
- **Backend** : `glin-profanity` + **OpenAI Moderation API** (double validation)
- **D√©tection** : Obfuscation + contexte + intentions
- **Queue** : Redis pour contenus suspects
- **Dashboard** : Interface de mod√©ration manuelle

**Workflow am√©lior√©** :
1. ‚úÖ Utilisateur saisit le nom du totem
2. ‚úÖ Validation frontend (`glin-profanity`)
3. ‚úÖ Validation backend Tier 1 (`glin-profanity`)
4. ‚úÖ Si flagg√© : Rejet imm√©diat
5. üîÑ Si limite (score 0.5-0.7) : OpenAI Moderation API
6. üîÑ Si OpenAI flagge : Mise en queue de mod√©ration
7. üë§ Review manuelle : Approuver / Rejeter
8. ‚úÖ Si OK : Cr√©ation Atom + Triple

**Co√ªt estim√©** :
- OpenAI Moderation : ~$1-5/mois (pour 10k propositions)
- Redis : Gratuit (Render ou Upstash Free)
- **Total** : **~$5/mois**

---

## üîç Impl√©mentation technique

### 1. Schema de validation Zod (Frontend + Backend)

```typescript
// shared/validation/totem.schema.ts
import { z } from 'zod';
import { profanity } from '@2toad/profanity';

profanity.setLanguages(['en', 'fr']);

export const TotemNameSchema = z.string()
  .min(3, "Le nom doit contenir au moins 3 caract√®res")
  .max(50, "Le nom ne peut pas d√©passer 50 caract√®res")
  .regex(/^[a-zA-Z0-9\s√Ä-√ø\-']+$/, "Caract√®res alphanum√©riques uniquement")
  .refine(
    (name) => !profanity.exists(name),
    { message: "Le nom contient du contenu inappropri√©" }
  );

export const TotemDescriptionSchema = z.string()
  .min(10, "La description doit contenir au moins 10 caract√®res")
  .max(500, "La description ne peut pas d√©passer 500 caract√®res")
  .refine(
    (desc) => !profanity.exists(desc),
    { message: "La description contient du contenu inappropri√©" }
  );

export const TotemProposalSchema = z.object({
  founderId: z.string().uuid(),
  totemName: TotemNameSchema,
  totemType: z.enum(['Object', 'Animal', 'Trait', 'Universe']),
  description: TotemDescriptionSchema,
  imageUrl: z.string().url().optional()
});
```

---

### 2. Hook React pour validation temps r√©el

```typescript
// frontend/src/hooks/useProfanityCheck.ts
import { useState, useEffect } from 'react';
import { ProfanityEngine } from 'glin-profanity';

const profanityEngine = new ProfanityEngine({
  languages: ['en', 'fr'],
  sensitivity: 'high',
  obfuscationDetection: true
});

export function useProfanityCheck(text: string) {
  const [isProfane, setIsProfane] = useState(false);
  const [score, setScore] = useState(0);
  const [matches, setMatches] = useState<string[]>([]);

  useEffect(() => {
    if (!text) {
      setIsProfane(false);
      setScore(0);
      setMatches([]);
      return;
    }

    const result = profanityEngine.analyze(text);
    setIsProfane(result.isProfane);
    setScore(result.score);
    setMatches(result.matches || []);
  }, [text]);

  return { isProfane, score, matches };
}
```

**Usage dans un composant** :
```tsx
// frontend/src/components/TotemForm.tsx
import { useProfanityCheck } from '@/hooks/useProfanityCheck';

function TotemForm() {
  const [name, setName] = useState('');
  const { isProfane, matches } = useProfanityCheck(name);

  return (
    <div>
      <input
        value={name}
        onChange={(e) => setName(e.target.value)}
        className={isProfane ? 'border-red-500' : 'border-gray-300'}
      />
      {isProfane && (
        <p className="text-red-500 text-sm mt-1">
          Le nom contient du contenu inappropri√© : {matches.join(', ')}
        </p>
      )}
    </div>
  );
}
```

---

### 3. Endpoint Backend (Fastify)

```typescript
// backend/src/routes/moderation.ts
import { FastifyPluginAsync } from 'fastify';
import { ProfanityEngine } from 'glin-profanity';
import { z } from 'zod';

const profanityEngine = new ProfanityEngine({
  languages: ['en', 'fr'],
  sensitivity: 'high',
  obfuscationDetection: true
});

const CheckTextSchema = z.object({
  text: z.string().min(1).max(500)
});

export const moderationRoutes: FastifyPluginAsync = async (fastify) => {
  // Route pour v√©rifier un texte
  fastify.post('/api/moderation/check', {
    schema: {
      body: CheckTextSchema
    },
    config: {
      rateLimit: {
        max: 20,
        timeWindow: '1 minute'
      }
    }
  }, async (request, reply) => {
    const { text } = request.body;

    const result = profanityEngine.analyze(text);

    return reply.send({
      isProfane: result.isProfane,
      score: result.score,
      severity: result.severity,
      matches: result.matches,
      clean: !result.isProfane
    });
  });

  // Route pour nettoyer un texte
  fastify.post('/api/moderation/censor', {
    schema: {
      body: CheckTextSchema
    }
  }, async (request, reply) => {
    const { text } = request.body;

    const censored = profanityEngine.censor(text);

    return reply.send({
      original: text,
      censored
    });
  });
};
```

---

### 4. Middleware de mod√©ration automatique

```typescript
// backend/src/middleware/moderation.middleware.ts
import { FastifyRequest, FastifyReply } from 'fastify';
import { ProfanityEngine } from 'glin-profanity';

const profanityEngine = new ProfanityEngine({
  languages: ['en', 'fr'],
  sensitivity: 'high',
  obfuscationDetection: true
});

interface ModerationOptions {
  fields: string[]; // Champs √† v√©rifier
  autoReject: boolean; // Rejeter automatiquement ou mettre en queue
}

export function moderationMiddleware(options: ModerationOptions) {
  return async (request: FastifyRequest, reply: FastifyReply) => {
    const body = request.body as Record<string, any>;

    for (const field of options.fields) {
      const value = body[field];
      if (typeof value !== 'string') continue;

      const result = profanityEngine.analyze(value);

      if (result.isProfane) {
        if (options.autoReject) {
          return reply.status(400).send({
            error: 'CONTENT_MODERATION_FAILED',
            message: `Le champ "${field}" contient du contenu inappropri√©`,
            field,
            matches: result.matches,
            severity: result.severity
          });
        } else {
          // Mettre en queue de mod√©ration manuelle
          request.log.warn({
            event: 'content_flagged',
            field,
            score: result.score,
            matches: result.matches
          });
          // TODO: Ajouter √† la queue Redis
        }
      }
    }
  };
}
```

**Usage** :
```typescript
// backend/src/routes/totem.ts
import { moderationMiddleware } from '@/middleware/moderation.middleware';

fastify.post('/api/totem/propose', {
  preHandler: moderationMiddleware({
    fields: ['totemName', 'description'],
    autoReject: true
  })
}, async (request, reply) => {
  // Si on arrive ici, le contenu est clean
  // Cr√©er l'Atom + Triple
});
```

---

### 5. Int√©gration OpenAI Moderation (Phase 2)

```typescript
// backend/src/services/moderation.service.ts
import OpenAI from 'openai';
import { ProfanityEngine } from 'glin-profanity';

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY
});

const profanityEngine = new ProfanityEngine({
  languages: ['en', 'fr'],
  sensitivity: 'high',
  obfuscationDetection: true
});

export interface ModerationResult {
  isClean: boolean;
  score: number;
  flaggedBy: ('profanity-filter' | 'openai' | 'both')[];
  categories: string[];
  needsManualReview: boolean;
}

export class ModerationService {
  /**
   * V√©rifie un texte avec approche multi-niveaux
   */
  async checkText(text: string): Promise<ModerationResult> {
    const result: ModerationResult = {
      isClean: true,
      score: 0,
      flaggedBy: [],
      categories: [],
      needsManualReview: false
    };

    // Tier 1 : Profanity filter (gratuit, rapide)
    const profanityResult = profanityEngine.analyze(text);

    if (profanityResult.isProfane) {
      result.isClean = false;
      result.score = Math.max(result.score, profanityResult.score);
      result.flaggedBy.push('profanity-filter');
      result.categories.push('profanity');

      // Si score tr√®s √©lev√©, rejet imm√©diat
      if (profanityResult.score > 0.8) {
        return result;
      }
    }

    // Tier 2 : OpenAI Moderation (payant, contextuel)
    try {
      const moderation = await openai.moderations.create({
        input: text
      });

      const aiResult = moderation.results[0];

      if (aiResult.flagged) {
        result.isClean = false;
        result.flaggedBy.push('openai');

        // Ajouter les cat√©gories d√©tect√©es
        Object.entries(aiResult.categories).forEach(([category, flagged]) => {
          if (flagged) {
            result.categories.push(category);
          }
        });

        // Score max des cat√©gories
        const maxScore = Math.max(...Object.values(aiResult.category_scores));
        result.score = Math.max(result.score, maxScore);
      }
    } catch (error) {
      // Fallback si OpenAI down
      console.error('OpenAI Moderation API error:', error);
    }

    // D√©cision finale
    if (result.score > 0.9) {
      // Rejet automatique
      result.needsManualReview = false;
    } else if (result.score > 0.5) {
      // Zone grise : review manuelle
      result.needsManualReview = true;
    }

    return result;
  }

  /**
   * Batch check pour plusieurs textes
   */
  async checkBatch(texts: string[]): Promise<ModerationResult[]> {
    return Promise.all(texts.map(text => this.checkText(text)));
  }
}
```

**Usage** :
```typescript
// backend/src/routes/totem.ts
import { ModerationService } from '@/services/moderation.service';

const moderationService = new ModerationService();

fastify.post('/api/totem/propose', async (request, reply) => {
  const { totemName, description } = request.body;

  // V√©rifier nom + description
  const [nameResult, descResult] = await moderationService.checkBatch([
    totemName,
    description
  ]);

  // Si l'un des deux est flagg√© avec score √©lev√©
  if (!nameResult.isClean && nameResult.score > 0.9) {
    return reply.status(400).send({
      error: 'CONTENT_MODERATION_FAILED',
      message: 'Le nom du totem contient du contenu inappropri√©',
      categories: nameResult.categories
    });
  }

  if (!descResult.isClean && descResult.score > 0.9) {
    return reply.status(400).send({
      error: 'CONTENT_MODERATION_FAILED',
      message: 'La description contient du contenu inappropri√©',
      categories: descResult.categories
    });
  }

  // Si zone grise (score 0.5-0.9), mettre en queue de review
  if (nameResult.needsManualReview || descResult.needsManualReview) {
    // Ajouter √† la queue Redis
    await addToModerationQueue({
      totemName,
      description,
      nameResult,
      descResult,
      userId: request.user.id,
      createdAt: new Date()
    });

    return reply.status(202).send({
      message: 'Votre proposition est en cours de mod√©ration',
      status: 'pending_review'
    });
  }

  // Sinon, cr√©er l'Atom + Triple
  // ...
});
```

---

## üîÑ Workflow de mod√©ration

### Workflow automatique (Phase 1)

```mermaid
graph TD
    A[Utilisateur soumet totem] --> B[Validation frontend glin-profanity]
    B -->|‚ùå Contenu inappropri√©| C[Message d'erreur]
    B -->|‚úÖ OK| D[Validation backend glin-profanity]
    D -->|‚ùå Contenu inappropri√©| E[Rejet HTTP 400]
    D -->|‚úÖ OK| F[Cr√©ation Atom + Triple]
    F --> G[Totem publi√©]
```

### Workflow semi-automatique (Phase 2)

```mermaid
graph TD
    A[Utilisateur soumet totem] --> B[Validation frontend]
    B -->|‚ùå| C[Message d'erreur]
    B -->|‚úÖ| D[Backend: glin-profanity]
    D -->|Score > 0.9| E[Rejet imm√©diat]
    D -->|Score 0.5-0.9| F[OpenAI Moderation API]
    D -->|Score < 0.5| G[Cr√©ation Atom]
    F -->|Flagg√©| H[Queue Redis - Pending Review]
    F -->|Clean| G
    H --> I[Mod√©rateur review manuelle]
    I -->|Approuv√©| G
    I -->|Rejet√©| J[Notification utilisateur]
    I -->|Bannir| K[Ban wallet + Notification]
```

---

## üñ•Ô∏è Interface de mod√©ration manuelle

### Dashboard de mod√©ration

**Fonctionnalit√©s requises** :
1. **Queue de contenus √† reviewer**
   - Tri par score de toxicit√© (priorit√©)
   - Filtrage par cat√©gorie
   - Filtrage par date
   - Recherche par wallet/utilisateur

2. **D√©tails de chaque contenu**
   - Nom du totem + description
   - Image (si pr√©sente)
   - Score de toxicit√©
   - Cat√©gories flagg√©es
   - Wallet de l'auteur
   - Historique de mod√©ration de cet utilisateur

3. **Actions disponibles**
   - ‚úÖ Approuver (cr√©er l'Atom)
   - ‚ùå Rejeter (avec raison)
   - üö´ Rejeter + Bannir utilisateur
   - üìù Demander modification

4. **Statistiques**
   - Nombre de contenus en attente
   - Taux d'approbation
   - Cat√©gories les plus flagg√©es
   - Temps moyen de review

---

### Interface React (exemple)

```tsx
// frontend/src/pages/ModerationDashboard.tsx
import { useState, useEffect } from 'react';
import { useQuery, useMutation } from '@tanstack/react-query';

interface PendingContent {
  id: string;
  totemName: string;
  description: string;
  imageUrl?: string;
  authorWallet: string;
  score: number;
  categories: string[];
  flaggedBy: string[];
  createdAt: string;
}

function ModerationDashboard() {
  const { data: pendingContents, refetch } = useQuery({
    queryKey: ['moderation', 'pending'],
    queryFn: () => fetch('/api/moderation/pending').then(r => r.json())
  });

  const approveMutation = useMutation({
    mutationFn: (id: string) =>
      fetch(`/api/moderation/${id}/approve`, { method: 'POST' }),
    onSuccess: () => refetch()
  });

  const rejectMutation = useMutation({
    mutationFn: ({ id, reason }: { id: string; reason: string }) =>
      fetch(`/api/moderation/${id}/reject`, {
        method: 'POST',
        body: JSON.stringify({ reason })
      }),
    onSuccess: () => refetch()
  });

  return (
    <div className="p-6">
      <h1 className="text-2xl font-bold mb-4">
        Mod√©ration - {pendingContents?.length || 0} contenus en attente
      </h1>

      <div className="space-y-4">
        {pendingContents?.map((content: PendingContent) => (
          <div key={content.id} className="border rounded-lg p-4 bg-white shadow">
            {/* Score et priorit√© */}
            <div className="flex justify-between items-start mb-4">
              <div>
                <h3 className="font-bold text-lg">{content.totemName}</h3>
                <p className="text-sm text-gray-600">
                  Par {content.authorWallet.slice(0, 6)}...{content.authorWallet.slice(-4)}
                </p>
              </div>
              <div className="text-right">
                <div className={`font-bold ${
                  content.score > 0.8 ? 'text-red-600' :
                  content.score > 0.5 ? 'text-orange-500' :
                  'text-yellow-500'
                }`}>
                  Score: {(content.score * 100).toFixed(0)}%
                </div>
                <div className="text-xs text-gray-500">
                  {new Date(content.createdAt).toLocaleDateString('fr-FR')}
                </div>
              </div>
            </div>

            {/* Description */}
            <p className="text-sm mb-4 p-3 bg-gray-50 rounded">
              {content.description}
            </p>

            {/* Image si pr√©sente */}
            {content.imageUrl && (
              <img
                src={content.imageUrl}
                alt={content.totemName}
                className="w-32 h-32 object-cover rounded mb-4"
              />
            )}

            {/* Cat√©gories flagg√©es */}
            <div className="flex flex-wrap gap-2 mb-4">
              {content.categories.map(cat => (
                <span key={cat} className="px-2 py-1 bg-red-100 text-red-800 text-xs rounded">
                  {cat}
                </span>
              ))}
            </div>

            {/* Flagg√© par */}
            <div className="text-xs text-gray-600 mb-4">
              Flagg√© par : {content.flaggedBy.join(', ')}
            </div>

            {/* Actions */}
            <div className="flex gap-2">
              <button
                onClick={() => approveMutation.mutate(content.id)}
                className="px-4 py-2 bg-green-500 text-white rounded hover:bg-green-600"
              >
                ‚úÖ Approuver
              </button>
              <button
                onClick={() => rejectMutation.mutate({
                  id: content.id,
                  reason: 'Contenu inappropri√©'
                })}
                className="px-4 py-2 bg-red-500 text-white rounded hover:bg-red-600"
              >
                ‚ùå Rejeter
              </button>
              <button
                onClick={() => {/* TODO: Modal pour bannir */}}
                className="px-4 py-2 bg-gray-800 text-white rounded hover:bg-gray-900"
              >
                üö´ Bannir utilisateur
              </button>
            </div>
          </div>
        ))}
      </div>
    </div>
  );
}
```

---

### API Backend pour le dashboard

```typescript
// backend/src/routes/moderation-admin.ts
import { FastifyPluginAsync } from 'fastify';
import Redis from 'ioredis';

const redis = new Redis(process.env.REDIS_URL);

export const moderationAdminRoutes: FastifyPluginAsync = async (fastify) => {
  // Middleware : V√©rifier que l'utilisateur est admin/mod√©rateur
  fastify.addHook('preHandler', async (request, reply) => {
    // TODO: V√©rifier r√¥le admin
    // if (!request.user.isAdmin) {
    //   return reply.status(403).send({ error: 'Forbidden' });
    // }
  });

  // Liste des contenus en attente
  fastify.get('/api/moderation/pending', async (request, reply) => {
    const pendingKeys = await redis.keys('moderation:pending:*');

    const contents = await Promise.all(
      pendingKeys.map(async (key) => {
        const data = await redis.get(key);
        return JSON.parse(data);
      })
    );

    // Trier par score (priorit√© haute d'abord)
    contents.sort((a, b) => b.score - a.score);

    return reply.send(contents);
  });

  // Approuver un contenu
  fastify.post('/api/moderation/:id/approve', async (request, reply) => {
    const { id } = request.params as { id: string };

    // R√©cup√©rer les donn√©es
    const data = await redis.get(`moderation:pending:${id}`);
    if (!data) {
      return reply.status(404).send({ error: 'Content not found' });
    }

    const content = JSON.parse(data);

    // Cr√©er l'Atom + Triple sur INTUITION
    // TODO: Appeler SDK INTUITION pour cr√©er

    // Supprimer de la queue
    await redis.del(`moderation:pending:${id}`);

    // Logger l'action
    await redis.lpush('moderation:logs', JSON.stringify({
      action: 'approved',
      contentId: id,
      moderator: request.user.wallet,
      timestamp: new Date()
    }));

    // Notifier l'utilisateur
    // TODO: Notification

    return reply.send({ success: true });
  });

  // Rejeter un contenu
  fastify.post('/api/moderation/:id/reject', async (request, reply) => {
    const { id } = request.params as { id: string };
    const { reason } = request.body as { reason: string };

    // R√©cup√©rer les donn√©es
    const data = await redis.get(`moderation:pending:${id}`);
    if (!data) {
      return reply.status(404).send({ error: 'Content not found' });
    }

    const content = JSON.parse(data);

    // Supprimer de la queue
    await redis.del(`moderation:pending:${id}`);

    // Archiver dans rejected
    await redis.lpush('moderation:rejected', JSON.stringify({
      ...content,
      rejectedBy: request.user.wallet,
      rejectedAt: new Date(),
      reason
    }));

    // Logger
    await redis.lpush('moderation:logs', JSON.stringify({
      action: 'rejected',
      contentId: id,
      moderator: request.user.wallet,
      reason,
      timestamp: new Date()
    }));

    // Notifier l'utilisateur
    // TODO: Notification

    return reply.send({ success: true });
  });

  // Bannir un utilisateur
  fastify.post('/api/moderation/:id/ban', async (request, reply) => {
    const { id } = request.params as { id: string };
    const { permanent, duration } = request.body as { permanent: boolean; duration?: number };

    // R√©cup√©rer les donn√©es
    const data = await redis.get(`moderation:pending:${id}`);
    if (!data) {
      return reply.status(404).send({ error: 'Content not found' });
    }

    const content = JSON.parse(data);

    // Bannir le wallet
    if (permanent) {
      await redis.sadd('moderation:banned', content.authorWallet);
    } else {
      await redis.setex(
        `moderation:banned:${content.authorWallet}`,
        duration || 86400 * 7, // 7 jours par d√©faut
        '1'
      );
    }

    // Rejeter le contenu
    await redis.del(`moderation:pending:${id}`);

    // Logger
    await redis.lpush('moderation:logs', JSON.stringify({
      action: 'banned',
      wallet: content.authorWallet,
      moderator: request.user.wallet,
      permanent,
      duration,
      timestamp: new Date()
    }));

    // Notifier l'utilisateur
    // TODO: Notification

    return reply.send({ success: true });
  });

  // Statistiques
  fastify.get('/api/moderation/stats', async (request, reply) => {
    const pendingCount = await redis.keys('moderation:pending:*').then(k => k.length);
    const bannedCount = await redis.scard('moderation:banned');
    const logsCount = await redis.llen('moderation:logs');

    // Logs des 7 derniers jours
    const logs = await redis.lrange('moderation:logs', 0, -1);
    const parsedLogs = logs.map(l => JSON.parse(l));

    const last7Days = parsedLogs.filter(log => {
      const date = new Date(log.timestamp);
      const now = new Date();
      return (now.getTime() - date.getTime()) < 7 * 24 * 60 * 60 * 1000;
    });

    const approved = last7Days.filter(l => l.action === 'approved').length;
    const rejected = last7Days.filter(l => l.action === 'rejected').length;
    const banned = last7Days.filter(l => l.action === 'banned').length;

    return reply.send({
      pending: pendingCount,
      totalBanned: bannedCount,
      totalLogs: logsCount,
      last7Days: {
        approved,
        rejected,
        banned,
        approvalRate: approved / (approved + rejected) * 100
      }
    });
  });
};
```

---

## üí∞ Co√ªts et maintenance

### Phase 1 : MVP (Gratuit)

| Service | Co√ªt | Notes |
|---------|------|-------|
| `glin-profanity` | $0 | Open-source |
| Fastify backend | $0 | Render Free Tier |
| Frontend React | $0 | Vercel/Netlify Free |
| **Total** | **$0/mois** | ‚úÖ |

---

### Phase 2 : Production avec OpenAI

| Service | Co√ªt estim√© | Notes |
|---------|-------------|-------|
| `glin-profanity` | $0 | Open-source |
| OpenAI Moderation API | $1-5/mois | 10k-50k propositions |
| Redis (Upstash Free) | $0 | 10k requ√™tes/jour |
| Fastify backend | $7/mois | Render Starter (si > 750h/mois) |
| Frontend React | $0 | Vercel/Netlify Free |
| **Total** | **~$8-12/mois** | ‚úÖ Abordable |

---

### Phase 3 : Scale (Optionnel)

| Service | Co√ªt estim√© | Notes |
|---------|-------------|-------|
| `glin-profanity` | $0 | Open-source |
| OpenAI Moderation API | $10-30/mois | 100k+ propositions |
| Perspective API | $0 | Gratuit (sous quota) |
| Redis (Upstash Pro) | $10/mois | 100k requ√™tes/jour |
| Fastify backend | $25/mois | Render Standard |
| Frontend React | $20/mois | Vercel Pro |
| **Total** | **~$65-85/mois** | üîÑ Si croissance |

---

## ‚öñÔ∏è Conformit√© l√©gale

### GDPR (Europe)

- **Stockage des donn√©es** : Contenus mod√©r√©s stock√©s temporairement (7-30 jours max)
- **Droit √† l'oubli** : Utilisateurs peuvent demander suppression de leurs contenus rejet√©s
- **Transparence** : Utilisateurs inform√©s des raisons de mod√©ration

### Digital Services Act (DSA) - EU

- **Signalement** : Syst√®me de signalement clair
- **Recours** : Possibilit√© de contester une d√©cision de mod√©ration
- **Transparence** : Publier des rapports de mod√©ration (nombre de contenus mod√©r√©s, cat√©gories, etc.)

### Moderation Best Practices

1. **Coh√©rence** : Appliquer les r√®gles de mani√®re uniforme
2. **Rapidit√©** : R√©pondre rapidement aux contenus signal√©s (< 24h)
3. **Documentation** : Conserver logs de mod√©ration (audit trail)
4. **Formation** : Former les mod√©rateurs aux biais et faux positifs
5. **Feedback loop** : Utiliser les faux positifs pour am√©liorer les filtres

---

## üìã Plan d'impl√©mentation

### Phase 1 : MVP (1-2 semaines)

**Objectif** : Mod√©ration automatique gratuite

‚úÖ **√âtape 1** : Installation et configuration
```bash
pnpm add glin-profanity zod
```

‚úÖ **√âtape 2** : Sch√©ma de validation Zod
- Cr√©er `shared/validation/totem.schema.ts`
- Ajouter `TotemNameSchema`, `TotemDescriptionSchema`

‚úÖ **√âtape 3** : Hook React frontend
- Cr√©er `useProfanityCheck.ts`
- Int√©grer dans `TotemForm.tsx`
- Feedback en temps r√©el

‚úÖ **√âtape 4** : Middleware backend
- Cr√©er `moderation.middleware.ts`
- Appliquer sur `/api/totem/propose`
- Rejet automatique si flagg√©

‚úÖ **√âtape 5** : Tests
- Tester avec mots interdits FR + EN
- Tester obfuscation (l33t, espaces)
- V√©rifier faux positifs (Phoenix, Arsenal, etc.)

---

### Phase 2 : Production avec OpenAI (2-3 semaines)

**Objectif** : Mod√©ration contextuelle + queue manuelle

‚úÖ **√âtape 1** : Setup OpenAI
```bash
pnpm add openai
```
- Obtenir API key OpenAI
- Configurer `.env`

‚úÖ **√âtape 2** : Service de mod√©ration
- Cr√©er `moderation.service.ts`
- Impl√©menter `checkText()` avec double validation
- D√©finir seuils (0.5 = queue, 0.9 = rejet)

‚úÖ **√âtape 3** : Queue Redis
```bash
pnpm add ioredis
```
- Setup Redis (Upstash Free ou Render)
- Cr√©er keys `moderation:pending:*`
- Impl√©menter `addToModerationQueue()`

‚úÖ **√âtape 4** : Dashboard de mod√©ration
- Cr√©er `ModerationDashboard.tsx`
- Routes API `/api/moderation/pending`, `/approve`, `/reject`, `/ban`
- Afficher score, cat√©gories, actions

‚úÖ **√âtape 5** : Notifications
- Notifier utilisateurs des d√©cisions
- Email ou notification on-chain

‚úÖ **√âtape 6** : Tests et monitoring
- Tester workflow complet
- Logs avec Pino
- Monitoring avec Sentry (optionnel)

---

### Phase 3 : Scale (optionnel, 1-2 semaines)

**Objectif** : Mod√©ration communautaire + ML custom

üîÑ **√âtape 1** : Signalement utilisateurs
- Bouton "Signaler" sur chaque totem
- Seuil de signalements pour queue automatique

üîÑ **√âtape 2** : R√©putation mod√©rateurs
- Score de pr√©cision des d√©cisions
- Gamification (badges, leaderboard)

üîÑ **√âtape 3** : ML custom (optionnel)
- Entra√Æner mod√®le sur faux positifs/n√©gatifs
- Am√©liorer d√©tection contextuelle

üîÑ **√âtape 4** : Analytics
- Dashboard de statistiques avanc√©es
- Trends de toxicit√© par p√©riode
- Export de rapports (DSA compliance)

---

## üéØ Checklist finale

### Configuration initiale
- [ ] Installer `glin-profanity`
- [ ] Configurer langues (FR + EN)
- [ ] Activer d√©tection obfuscation
- [ ] D√©finir sensibilit√© (high)

### Frontend
- [ ] Hook `useProfanityCheck`
- [ ] Validation en temps r√©el dans formulaire
- [ ] Messages d'erreur clairs
- [ ] Feedback visuel (border rouge, etc.)

### Backend
- [ ] Middleware de mod√©ration
- [ ] Endpoint `/api/moderation/check`
- [ ] Validation avant cr√©ation Atom
- [ ] Rate limiting sur endpoints mod√©ration

### Phase 2 (optionnel)
- [ ] Int√©gration OpenAI Moderation API
- [ ] Redis pour queue
- [ ] Dashboard de mod√©ration manuelle
- [ ] Routes `/approve`, `/reject`, `/ban`
- [ ] Notifications utilisateurs
- [ ] Logs et monitoring

### Tests
- [ ] Tester mots FR (insultes, jurons)
- [ ] Tester mots EN (profanity, slurs)
- [ ] Tester obfuscation (l33t, espaces, Unicode)
- [ ] V√©rifier faux positifs (noms l√©gitimes)
- [ ] Tester rate limiting

### Documentation
- [ ] Documenter r√®gles de mod√©ration
- [ ] Cr√©er FAQ pour utilisateurs
- [ ] Guidelines pour mod√©rateurs
- [ ] Conformit√© GDPR/DSA

---

## üìù Ressources et r√©f√©rences

### Packages npm
- [glin-profanity](https://www.npmjs.com/package/glin-profanity) - Recommand√© ‚≠ê
- [@2toad/profanity](https://www.npmjs.com/package/@2toad/profanity)
- [leo-profanity](https://www.npmjs.com/package/leo-profanity)
- [content-checker](https://www.npmjs.com/package/content-checker)

### APIs de mod√©ration
- [Perspective API](https://perspectiveapi.com/) - Google Jigsaw (gratuit)
- [OpenAI Moderation API](https://platform.openai.com/docs/guides/moderation) - $0.02/1k tokens
- [AWS Comprehend Toxicity](https://aws.amazon.com/comprehend/) - $0.0001/unit√© (EN uniquement)

### Documentation
- [OWASP Content Moderation](https://owasp.org/)
- [Digital Services Act (EU)](https://digital-strategy.ec.europa.eu/en/policies/digital-services-act-package)
- [GDPR Compliance](https://gdpr.eu/)

### Communaut√©
- [Content Moderation Best Practices](https://www.eff.org/deeplinks/2019/04/content-moderation)
- [Reddit - Content Moderation](https://www.reddit.com/r/ModSupport/)

---

## üèÅ Conclusion

### Recommandation finale

**Pour le MVP** :
- ‚úÖ **glin-profanity** (gratuit, multilingue, obfuscation)
- ‚úÖ Validation frontend + backend
- ‚úÖ Rejet automatique des contenus flagg√©s
- ‚úÖ **Co√ªt : $0/mois**

**Pour la production** :
- ‚úÖ **glin-profanity** + **OpenAI Moderation API**
- ‚úÖ Queue Redis pour review manuelle
- ‚úÖ Dashboard de mod√©ration
- ‚úÖ **Co√ªt : ~$8-12/mois**

### Prochaines √©tapes

1. ‚úÖ Impl√©menter Phase 1 (MVP)
2. ‚è≥ Tester avec utilisateurs beta
3. ‚è≥ Collecter faux positifs/n√©gatifs
4. ‚è≥ D√©cider si Phase 2 n√©cessaire
5. ‚è≥ Monitoring et am√©lioration continue

---

**Derni√®re mise √† jour** : 18 novembre 2025
**Auteur** : Documentation Master - INTUITION Founders Totem
**Statut** : ‚úÖ Complet
